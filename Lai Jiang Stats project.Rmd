---
title: "Stats Takehome project"
author: "Lai Jiang"
date: "8/17/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
###########################################################################################################################
##Visual Story Telling Part 1: Green Building

```{r}
library(corrplot)
library(LICORS)
library(RColorBrewer)
library(mosaic)
library(tidyverse)
```

```{r}
urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv"
greenbuildings = read.csv(url(urlfile))
```
```{r}
greenbuildings$green_rating<-as.factor(greenbuildings$green_rating)
##median rent for green building
median(greenbuildings$Rent[greenbuildings$green_rating==1])

##median rent for non-green building
median(greenbuildings$Rent[greenbuildings$green_rating==0])
```
##indeed 2.6 dollar more on average per square ft

## visualizations
```{r}
ggplot(data=greenbuildings) + 
  geom_point(mapping=aes(x=cluster_rent, y=Rent, color=green_rating)) +
  labs(x="Cluster Rent", y='Rent', title = 'Green buildings: Cluster rent VS Rent',
       color='Green building')
ggplot(data=greenbuildings) + 
  geom_point(mapping=aes(x=size, y=Rent, color=green_rating)) +
  labs(x="Size", y='Rent', title = 'Green buildings: Size VS Rent',
       color='Green building') 
ggplot(data=greenbuildings) + 
  geom_point(mapping=aes(x=leasing_rate, y=Rent, color=green_rating)) +
  labs(x="leasing_rate", y='Rent', title = 'Green buildings: leasing rate VS Rent',
       color='Green building')
ggplot(data=greenbuildings) + 
  geom_point(mapping=aes(x=age, y=Rent, colour=class_a))+
  labs(x="Age", y='Rent', title = 'Class A: Age VS Rent',
       color='Class A building')
```

## Observaton:
## Class A building for the same age old are charged a higher rent than Non-class A buildings
## Rent and Cluster rent is postively correlated. 
## Lease rate is poitvely correlated with rent.
## Size is correlated with Rent as expected

```{r}
g = ggplot(greenbuildings, aes(x=age))
g + geom_density(aes(fill=factor(green_rating)), alpha=0.4)+
  labs(x="Age", y='Density', title = 'Distribution of age',
       fill='Green building')

ggplot(greenbuildings, aes(class_a, ..count..)) + geom_bar(aes(fill = green_rating), position = "stack")+
  labs(x="Class a", y='Number of buildings', title = 'Class A vs Green Buildings',
       fill='Green building')

g = ggplot(greenbuildings, aes(x=size))
g + geom_density(aes(fill=factor(green_rating)), alpha=0.4)+
  labs(x="Size", y='Density', title = 'Distribution of size',
       fill='Green building')
```
## Most of the green buildings are built later than non-green buildings
## More green buildings are in the class A buildings
## more green builings are smaller in size <50
```{r}
greenbuildings$age_cut <- cut(greenbuildings$age, breaks = c(0, seq(10, 190, by = 10)), labels = 0:18,right=FALSE)
medians <- aggregate(Rent~ age_cut + green_rating, greenbuildings, median)
ggplot(data = medians, mapping = aes(y = Rent, x = age_cut ,group = green_rating, color=green_rating)) +
   geom_line(size=1.2) +
  labs(x="Age in 10 years", y='Median Rent', title = 'All buildings: Median rent over the years',
       fill='Green building')
```
## again, green buildings are much newer and within 11 years of building age and charge a much higher rent on average between building age 10 to building age 90 years 

# Size cut in 100k buckets
```{r}
greenbuildings$size_cut <- cut(greenbuildings$size, breaks = c(0, seq(10, 3781045, by = 100000)), labels = 0:37,right=FALSE)
medians <- aggregate(Rent ~ size_cut + green_rating, greenbuildings, median)
ggplot(data = medians, mapping = aes(y = Rent, x = size_cut ,group = green_rating, color=green_rating)) +
   geom_line(size=1.5) +
  labs(x="Size in 100k sq.ft", y='Median Rent', title = 'All buildings: median rent for different building sizes',
       fill='Green building')
```
## once the building increase pass 1800K sqft. the non_green buildings has much higher median rent.

```{r}
nonclass_a <- subset(greenbuildings, greenbuildings$class_a != 1)
nonclass_a$age_cut <- cut(nonclass_a$age, breaks = c(0, seq(10, 190, by = 10)), labels = 0:18,right=FALSE)
medians <- aggregate(Rent~ age_cut + green_rating, nonclass_a, median)
ggplot(data = medians, mapping = aes(y = Rent, x = age_cut ,group = green_rating, color=green_rating)) +
   geom_line(size=1.5)+
  labs(x="Age in 10 years", y='Median Rent', title = 'Non-Class A buildings: Median rent over the years',
       fill='Green building')
```
## Non-class A green buildings only have a higher median rent than non-class A non green building if it's 20-30 years older or 40-90 years old.


# Size in 100k
```{r}
nonclass_a$size_cut <- cut(nonclass_a$size, breaks = c(0, seq(10, 3781045, by = 100000)), labels = 0:37,right=FALSE)
medians <- aggregate(Rent ~ size_cut + green_rating, nonclass_a, median)
ggplot(data = medians, mapping = aes(y = Rent, x = size_cut ,group = green_rating, colour=green_rating)) +
   geom_line(size=1.5)+
  labs(x="Size in 100k sq.ft", y='Median Rent', title = 'Non-class A buildings: median rent for different building sizes',
       fill='Green building')  
```
## In non-class A buildings, median rent of green buildings is lower than non-green ones

```{r}
data_size <- subset(greenbuildings, greenbuildings$size > 200000 & greenbuildings$size < 300000)
data_size <- subset(greenbuildings, greenbuildings$class_a == 1)
data_size_class <- subset(greenbuildings, nonclass_a$size > 200000 & nonclass_a$size < 300000)
paste("Median leasing rate for class a buildings of sizes ranging from 200k to 300k sq.ft ", 
median(data_size$leasing_rate))
medians <- aggregate(Rent~ age_cut + green_rating, data_size, median)
medians_1 <- subset(medians, medians$green_rating == 1)
rent_1<-medians_1[1:5,]$Rent
medians_0 <- subset(medians, medians$green_rating == 0)
rent_0<-medians_0[1:5,]$Rent
paste("Difference in rent for the first 5 years class a buildings: ", 
(sum(rent_1,na.rm = T) - sum(rent_0, na.rm = T)) / 5)
medians <- aggregate(Rent~ age_cut + green_rating, data_size_class, median)
medians_1 <- subset(medians, medians$green_rating == 1)
rent_1<-medians_1[1:5,]$Rent
medians_0 <- subset(medians, medians$green_rating == 0)
rent_0<-medians_0[1:5,]$Rent
paste("Difference in rent for the first 5 years for non-class a buildings: ", 
(sum(rent_1,na.rm = T) - sum(rent_0, na.rm = T)) / 5 )
```
## the analyst fails to account other factors such as building class, size in his analysis.
## rent differs at different age and size, since we are looking at 250K sq.ft investment. We should focus on 200-300K sqft
## We will use average 5 years return to arrive at final recommendation since data does not having information about class A buildings size from 200K to 300K sq ft.

```{r}
paste("If we build a class a green building and if we assume 91.6% occpancy rate, it is expected to recuperate the costs in  ", round(5000000/(3.848*250000*0.9289),2), " years")
```
## Recommendation:
## Do not invest in non Class A building since difference in rent for first 5 years for green vs non-green in non-class A is -1.167
## expect a 92.89% occupancy rate
## Difference in rent for the first 5 years for green vs non-green class a buildings is 3.848
## we recooperate the 5mil extra investment for green certification in 5.6 years.



######################################################################################################################################################################################################################################################
```{r}
library(ggplot2)
library(ggpubr)
urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv"
airport = read.csv(url(urlfile),header = TRUE)
head(airport)
attach(airport)
colSums(is.na(airport))
```
```{r}
colnames(airport)
dim(airport)
```

```{r}
airport[is.na(airport)] <- 0
colSums(is.na(airport))
col_factors <- c('Month', 'DayofMonth', 'DayOfWeek', 'Cancelled', 'Diverted')
airport[,col_factors] <- lapply(airport[,col_factors], as.factor)
airport$Dep_Hr <- sapply(DepTime, function(x) x%/%100)
airport$CRSDep_Hr <- sapply(CRSDepTime, function(x) x%/%100)
airport$Arr_Hr <- sapply(ArrTime, function(x) x%/%100)
airport$CRSArr_Hr <- sapply(CRSArrTime, function(x) x%/%100)
aus.dep <- subset(airport, Origin == 'AUS')
aus.arr <- subset(airport, Dest == 'AUS')
```
```{r}
pl <- ggplot(data = airport, aes(x=ArrDelay)) + 
  geom_histogram(bins = 100, binwidth = 10, color='blue') + 
  xlab('Arrival Delay') +
  ggtitle('Distribution of Arrival Delays')
print(pl)
pl2 <- ggplot(data=airport, aes(x=))
```
```{r}
pl <- ggplot(data = airport, aes(x=ArrDelay)) + 
  geom_histogram(bins = 100, binwidth = 10, color='blue') + 
  xlab('Arrival Delay') +
  ggtitle('Distribution of Arrival Delays')
print(pl)
pl2 <- ggplot(data=airport, aes(x=))
```
##arrival delays and departure delays looks about the same centered around zero.


```{r}
pl <- ggplot(aes(x=UniqueCarrier), data=airport) +
  geom_bar(fill='black', position='dodge') +
  ggtitle('Number of operations by Carrier') +
  xlab('Carrier Name') +
  ylab('Number of operations')
  
print(pl)
```

##Southwest(WN) tops the list with almost 40k operations, followed by Alaskan Airlines(AA). Northwest Airlines(NW) has the fewest operations 



## Correlation of arrival and departure delays.There are a few outliers, but it is a good straight line curve showing
## they are directly correlated

```{r, echo=FALSE}
pl <- ggplot(aes(x=DepDelay, y=ArrDelay), data=airport) +
  geom_point(aes(color=UniqueCarrier))
print(pl +
        ggtitle('Correlation between arrival and departure delays') +
        xlab('Departure Delay') +
        ylab('Arrival Delay'))
```


We will try to visualize the correlation for carriers individually. Visually, the slope of the plots remain roughly constant, so NO carriers are consistently making up for the departure delays.

```{r fig.width=6, fig.height=20, echo=FALSE}
  
pl <- ggplot(aes(x=DepDelay, y=ArrDelay), data=airport) +
  geom_point() +
  facet_grid(UniqueCarrier ~ .) +
  ggtitle('Arrival delay and depature delay correlation by carrier')
print(pl)
```
## the slope of the plots remain roughly constant for each individual airlines. if the plane arrived late, they will likely to depart late

######################################################################################################################################################################################################################################################
## Portfolio Modeling
```{r}
## Portfolio Modeling
library(mosaic)
library(quantmod)
library(foreach)
```
## random Portfolio 1
## IXC: iShares S&P Global Energy Sector
## GOEX: GLB X FUNDS/GLB X GOLD EXPLORER
## Vanguard Energy ETF

```{r}
mystocks = c("IXC", "GOEX", "VDE")
myprices = getSymbols(mystocks, from = "2015-08-04")
# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
```


```{r}
# Combine all the returns in a matrix
all_returns = cbind(ClCl(IXCa),
								ClCl(GOEXa),
								ClCl(VDEa))
head(all_returns)
#remove the first day (no return calculated)
all_returns = as.matrix(na.omit(all_returns))
head(all_returns)

pairs(all_returns)
```
##We see some trends here. VDE and IXC seems to be positively correlated. the return level of IXC and GOEX seems cluster together
```{r}
initial_wealth = 10000
set.seed(1000)
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.5, 0.3, 0.2)
	holdings = weights * total_wealth
	n_days = 20 #capital T in the notes
	wealthtracker = rep(0, n_days) #setup a place holder to tracl total wealth
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		holdings = weights * total_wealth ##rebalance portfolios
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
```

```{r}
head(sim1)
hist(sim1[,n_days], 25)

## average total wealth after 20 days
mean(sim1[,n_days]) ## average total wealth after 20 days
## average gains after 20days
mean(sim1[,n_days] - initial_wealth) 
## histogram of gains distribution
hist(sim1[,n_days]- initial_wealth, breaks=30) #histogram of gains distribution


# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)
``` 
## we are 95% confident that our worst daily loss will not exceed 1254.1 at c(0.5, 0.3, 0.2) for "IXC", "GOEX", "VDE".

#########################################################################################################################

## random Portfolio 2
## AIA: iShares S&P Global Energy Sector
## RING: GLB X FUNDS/GLB X GOLD EXPLORER
## CSD:Invesco S&P Spin-Off ETF	


```{r}
mystocks = c("AIA", "RING", "CSD")
myprices = getSymbols(mystocks, from = "2015-08-04")
# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
```

```{r}
all_returns = cbind(ClCl(AIAa),
								ClCl(RINGa),
								ClCl(CSDa))
head(all_returns)
#remove the first day (no return calculated)
all_returns = as.matrix(na.omit(all_returns))
head(all_returns)
## showing pair wise stock return correlations
pairs(all_returns)
```

```{r}
initial_wealth = 10000
set.seed(1000)
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.3, 0.05,0.65)
	holdings = weights * total_wealth
	n_days = 20 #capital T in the notes
	wealthtracker = rep(0, n_days) #setup a place holder to tracl total wealth
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = weights * total_wealth
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		holdings = weights * total_wealth
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
```

```{r}
hist(sim1[,n_days], 25)
## average total wealth after 20 days
mean(sim1[,n_days]) ## average total wealth after 20 days
## average gains after 20days
mean(sim1[,n_days] - initial_wealth) 
## histogram of gains distribution
hist(sim1[,n_days]- initial_wealth, breaks=30) #histogram of gains distribution
# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)
```
## we are 95% confident that our worst daily loss will not exceed 1045.881 at c(0.5, 0.3, 0.2) for "IXC", "GOEX", "VDE".

#########################################################################################################################
#### let's try a 4 ETFS porfolios

## random Portfolio 3
## SLYV: SPDR S&P 600 Small Cap Value ETF
## EES: WisdomTree U.S. SmallCap Earnings Fund
## VIOV:Vanguard S&P Small-Cap 600 Value Index Fund ETF Shares
## FXZ: First Trust Materials AlphaDEX Fund

```{r}
mystocks = c("SLYV", "EES", "VIOV","FXZ")
myprices = getSymbols(mystocks, from = "2015-08-04")
# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
```

```{r}
all_returns = cbind(ClCl(SLYVa),
								ClCl(EESa),
								ClCl(VIOVa),
								ClCl(FXZa))
head(all_returns)
#remove the first day (no return calculated)
all_returns = as.matrix(na.omit(all_returns))
head(all_returns)
## showing pair wise stock return correlations
pairs(all_returns)
```

```{r}
all_returns = cbind(ClCl(SLYVa),
								ClCl(EESa),
								ClCl(VIOVa),
								ClCl(FXZa))
head(all_returns)
#remove the first day (no return calculated)
all_returns = as.matrix(na.omit(all_returns))
head(all_returns)
## showing pair wise stock return correlations
pairs(all_returns)
```

```{r}
initial_wealth = 10000
set.seed(1000)
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.25,0.25,0.25,0.25) #equal 25% weight
	holdings = weights * total_wealth
	n_days = 20 #capital T in the notes
	wealthtracker = rep(0, n_days) #setup a place holder to tracl total wealth
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		holdings = weights * total_wealth
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
```

```{r}
## average total wealth after 20 days
mean(sim1[,n_days]) ## average total wealth after 20 days
## average gains after 20days
mean(sim1[,n_days] - initial_wealth) 
## histogram of gains distribution
hist(sim1[,n_days]- initial_wealth, breaks=30) #histogram of gains distribution
# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05)
```

## we are 95% confident that our worst daily loss will not exceed 1103.32 at c(0.5, 0.3, 0.2) for "IXC", "GOEX", "VDE".


```{r}
library(ggplot2)
library(ggthemes)
library(reshape2)
library(RCurl)
library(foreach)
library(fpc)
library(cluster)
urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/social_marketing.csv"
social_m_raw <- read.csv(url(urlfile))
social_m <- read.csv(url(urlfile))
```

```{r,echo = FALSE, include=FALSE}
# Remove chatter and spam
social_m$chatter<- NULL
social_m$spam <- NULL
social_m$adult <- NULL
social_m$photo_sharing <- NULL 
social_m$health_nutrition <- NULL 
# Center and scale the data
X = social_m[,(2:32)]
X = scale(X, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
```

```{r, echo = FALSE, include=FALSE}
# Determine number of clusters
#Elbow Method for finding the optimal number of clusters
set.seed(123)
# Compute and plot wss for k = 2 to k = 15.
k.max <- 15
data <- X 
wss <- sapply(1:k.max, 
              function(k){kmeans(data, k, nstart=50,iter.max = 15 )$tot.withinss})
wss
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```



```{r, echo = FALSE, include=FALSE}
# Run k-means with 10 clusters and 25 starts
clust1 = kmeans(X, 10, nstart=25)
#hard to visualized
social_clust1 <- cbind(social_m, clust1$cluster)
```


```{r echo=FALSE, include=FALSE}
plotcluster(social_m[,2:32], clust1$cluster)
```



```{r,echo = FALSE, include=FALSE}
#cluster info to main data 
social_clust1_main <- as.data.frame(cbind(clust1$center[1,]*sigma + mu, 
                            clust1$center[2,]*sigma + mu,
                            clust1$center[3,]*sigma + mu,
                            clust1$center[4,]*sigma + mu,
                            clust1$center[5,]*sigma + mu,
                            clust1$center[6,]*sigma + mu,
                            clust1$center[7,]*sigma + mu,
                            clust1$center[8,]*sigma + mu,
                            clust1$center[9,]*sigma + mu,
                            clust1$center[10,]*sigma + mu))
summary(social_clust1_main)
#Change column names
names(social_clust1_main) <- c('Cluster_1',
                'Cluster_2',
                'Cluster_3',
                'Cluster_4',
                'Cluster_5',
                'Cluster_6',
                'Cluster_7',
                'Cluster_8',
                'Cluster_9',
                'Cluster_10')
# Must remove spam since it is the lowest in all 
# similarly chatter appears in all the cluster with high values
```



```{r out.width=c('50%', '50%'), fig.show='hold',echo = FALSE, include=FALSE}
#df1 <- melt(social_clust1_main,"row.names")
social_clust1_main$type <- row.names(social_clust1_main)
#Cluster 1
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_1) , y=Cluster_1)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 1",
        x ="Category", y = "Cluster centre values")
#cluster 2 
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_2) , y=Cluster_2)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 2",
        x ="Category", y = "Cluster centre values")
#Cluster 3
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_3) , y=Cluster_3)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 3",
        x ="Category", y = "Cluster centre values")
#Cluster 4
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_4) , y=Cluster_4)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 4",
        x ="Category", y = "Cluster centre values")
#cluster 5
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_5) , y=Cluster_5)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 5",
        x ="Category", y = "Cluster centre values")
#cluster 6
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_6) , y=Cluster_6)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 6",
        x ="Category", y = "Cluster centre values")
#Cluster 7
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_7) , y=Cluster_7)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 7",
        x ="Category", y = "Cluster centre values")
#Cluster 8
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_8) , y=Cluster_8)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 8",
        x ="Category", y = "Cluster centre values")
#Cluster 9
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_9) , y=Cluster_9)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 9",
        x ="Category", y = "Cluster centre values")
#Cluster 10
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_10) , y=Cluster_10)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 10",
        x ="Category", y = "Cluster centre values") 
#+xlab("Category") + ylab("Cluster centre values") + title("Cluster 1")
 # scale_x_discrete(limits = Cluster_)
```


## Market segmentation


## Steps taken


# K-means with the raw data 
# K-means with k-means++ initialization 
# K-means with k-means++ initialization using PCA data
# Hierarchial clustering using PCA data 

 
## Correlation plot

```{r, echo=FALSE, include=FALSE}
library('corrplot')
```

```{r, echo=FALSE}
cormat <- round(cor(social_m_raw[,2:37]), 2)
corrplot(cormat, method="circle")
```

#personal fitness and health nutrition are highly correlated. online gaming and college university variables are correlated.


## PCA

```{r, echo=FALSE, include=FALSE}
social_m_raw$chatter<- NULL
social_m_raw$spam <- NULL
social_m_raw$adult <- NULL
social_m_raw$photo_sharing <- NULL 
social_m_raw$health_nutrition <- NULL 
#################### PCA #########################
pca_sm = prcomp(social_m_raw[,2:32], scale=TRUE, center = TRUE)
summary(pca_sm)
#plot(pca_sm, type= 'l')
```


```{r, echo=FALSE}
pca_var <-  pca_sm$sdev ^ 2
pca_var1 <- pca_var / sum(pca_var)
#Cumulative sum of variation explained
plot(cumsum(pca_var1), xlab = "Principal Component", 
     ylab = "Fraction of variance explained")
```

```{r, echo=TRUE}
cumsum(pca_var1)[10]
```

## At 10th PC, around 63.37% of the variation is explained. According to Kaiser criterion, we should drop all the principal components with eigen values less than 1.0. Hence, let's pick 10 principal components. 

```{r, echo=FALSE, include=FALSE}
varimax(pca_sm$rotation[, 1:11])$loadings
```


```{r, echo=FALSE}
scores = pca_sm$x
pc_data <- as.data.frame(scores[,1:18])
X <- pc_data
```

## Kmean

```{r, echo=FALSE, include=FALSE}
library(LICORS)
```

```{r, echo=FALSE}
# Determine number of clusters
#Elbow Method for finding the optimal number of clusters
set.seed(123)
# Compute and plot wss for k = 2 to k = 15.
k.max <- 15
data <- X 
wss <- sapply(1:k.max, 
              function(k){kmeanspp(data, k, nstart=10,iter.max = 10 )$tot.withinss})
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```

##o use 5 cluster since it is easier to intrepret and identify market segments. 
```{r, echo=FALSE,include=FALSE}
# Run k-means with 10 clusters and 25 starts
clust1 = kmeanspp(X, 5, nstart=15)
#hard to visualized
social_clust1 <- cbind(social_m, clust1$cluster)
```


```{r, echo=FALSE, include=FALSE}
library(cluster)
library(HSAUR)
library(fpc)
```

## Cluster visualization

```{r, echo=FALSE, include=TRUE}
plotcluster(social_m[,2:32], clust1$cluster)
```

#The clusters look well separated. Let's identify the characteristics of the clusters. 


```{r, echo=FALSE, include=FALSE}
#cluster info to main data 
social_clust1_main <- as.data.frame(cbind(clust1$center[1,]*sigma + mu, 
                            clust1$center[2,]*sigma + mu,
                            clust1$center[3,]*sigma + mu,
                            clust1$center[4,]*sigma + mu,
                            clust1$center[5,]*sigma + mu))
summary(social_clust1_main)
#Change column names
names(social_clust1_main) <- c('Cluster_1',
                'Cluster_2',
                'Cluster_3',
                'Cluster_4',
                'Cluster_5')
                #'Cluster_6')
```

##Results

```{r out.width=c('50%', '50%'), fig.show='hold', echo=FALSE}
social_clust1_main$type <- row.names(social_clust1_main)
#Cluster 1
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_1) , y=Cluster_1)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 1",
        x ="Category", y = "Cluster centre values") 
#cluster 2 
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_2) , y=Cluster_2)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 2",
        x ="Category", y = "Cluster centre values")
#Cluster 3
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_3) , y=Cluster_3)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 3",
        x ="Category", y = "Cluster centre values")
#Cluster 4
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_4) , y=Cluster_4)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 4",
        x ="Category", y = "Cluster centre values")
#cluster 5
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_5) , y=Cluster_5)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 5",
        x ="Category", y = "Cluster centre values")
```


##Market segments identified 
##1. Sports Fandom, Travel, Cooking
##2. Crafts, Current Events
##3. TV Film, Automotive, Politics
##4. Cooking, Personal Fitness, Food, Shopping, Fashion
##5. Travel, Outdoors, Business

#Cluster 4 - "Cooking, Personal Fitness, Food, Shopping, Fashion" and Cluster 5 - "Travel, Outdoors, Business" differ vastly in terms of interests. Cluster 5 consists mainly of people who love travelling and people in Cluster 4 are more focused on personal grooming.
#Furthermore,Cluster in 1 is primarily composed of people who have a penchant for sports, travel and cooking. In contrast, cluster 2 has people who are artistic
#Cluster 3 seems like people with eclectic interests - starting from movies to automotive to politics and religion as well!

## Hierarchial Clustering
## Results

```{r, echo=FALSE, include=FALSE}
social_m <-social_m_raw
# Remove chatter and spam
social_m$chatter<- NULL
social_m$spam <- NULL
social_m$adult <- NULL
social_m$photo_sharing <- NULL 
social_m$health_nutrition <- NULL 
# Center and scale the data
X = social_m[,(2:32)]
X = scale(X, center=TRUE, scale=TRUE)
# Center/scale the data
#protein_scaled = scale(protein, center=TRUE, scale=TRUE) 
# Form a pairwise distance matrix using the dist function
protein_distance_matrix = dist(X, method='euclidean')
# Now run hierarchical clustering
hier_protein = hclust(protein_distance_matrix, method='average')
# Plot the dendrogram
#plot(hier_protein, cex=0.8)
# Cut the tree into 5 clusters
cluster1 = cutree(hier_protein, k=5)
summary(factor(cluster1))
```


```{r, echo=FALSE,include=FALSE }
X <- pc_data
# Form a pairwise distance matrix using the dist function
protein_distance_matrix = dist(X, method='euclidean')
# Now run hierarchical clustering
hier_protein = hclust(protein_distance_matrix, method='complete')
# Plot the dendrogram
#plot(hier_protein, cex=0.8)
# Cut the tree into 5 clusters
cluster1 = cutree(hier_protein, k=5)
summary(factor(cluster1))
```

```{r, echo=FALSE}
social_clust1 <- cbind(social_m, cluster1)
#social_m_hclust <- cbind(social_m,cluster1
```


```{r, echo=FALSE, include=FALSE}
hcluster_average <- aggregate(social_clust1, list(social_clust1$cluster1), mean)
hcluster_average$cluster1 <- paste("Cluster_", hcluster_average$cluster1, sep = '')
hcluster_average$Group.1 <- NULL
hcluster_average$X <- NULL
```


```{r, echo=FALSE}
row.names(hcluster_average) <- hcluster_average$cluster1
hcluster_average$cluster1 <- NULL
hcluster_average <- as.data.frame(t(hcluster_average))
```



```{r out.width=c('50%', '50%'), fig.show='hold', echo=FALSE}
hcluster_average$type <- row.names(hcluster_average)
social_clust1_main <- hcluster_average
#Cluster 1
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_1) , y=Cluster_1)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 1",
        x ="Category", y = "Cluster centre values")
#cluster 2 
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_2) , y=Cluster_2)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 2",
        x ="Category", y = "Cluster centre values")
#Cluster 3
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_3) , y=Cluster_3)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 3",
        x ="Category", y = "Cluster centre values")
#Cluster 4
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_4) , y=Cluster_4)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 4",
        x ="Category", y = "Cluster centre values")
#cluster 5
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_5) , y=Cluster_5)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 5",
        x ="Category", y = "Cluster centre values")
```



##Market segments identified
##1. Cooking, Personal Fitness
##2. Art, TV Film, Shopping
##3. Politics, Travel, Computers
##4. College Universities, Online Gaming, News
##5. Religion, Food, Parenting, School, Family

## Clustering reveals some interesting segments that differ by demographics. Intuitively, Cluster 4 - "College Universities, Online Gaming, News" will consist of a younger population as compared to Cluster 5 - "Religion, Food, Parenting, School, Family". NutrientH20 can design demographic specific marketing campaigns to get the most effective message across to their audience.
## Cluster 2 has a group of people who have artsy interests - art, tv film and travel. They sure do know how to enjoy life
## Cluster 3 has the computer lovers, who also have an interest for politics and travel. 



## Market segementation can allows us to derive insights that will help send the right message to the right group of people that will maximize the profits of the company and help build better relationships with the audience.





## Author Attribution
```{r, echo = FALSE,warning=FALSE,include=FALSE}
library(tm) 
library(magrittr)
library(slam)
library(proxy)
library(caret)
library(plyr)
library(dplyr)
library(ggplot2)
library('e1071')
```


## reading the files, processing data & Tokenization
```{r, echo = FALSE,warning=FALSE,include=FALSE}
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
```
## reading all the books in training set
```{r}							

train=Sys.glob('C:/Users/Lai Jiang/Desktop/UT MSBA Summer Courses/Predictive Modeling/STA380-James Scott/STA380-master/data/ReutersC50/C50train/*')
```

## Creating training dataset
```{r}
comb_art=NULL
labels=NULL
for (name in train)
{ 
  author=substring(name,first=50)
  article=Sys.glob(paste0(name,'/*.txt'))
  comb_art=append(comb_art,article)
  labels=append(labels,rep(author,length(article)))
}
```

## cleaning the file name

```{r}
readerPlain <- function(fname)
  {
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') 
  }
comb = lapply(comb_art, readerPlain) 
names(comb) = comb_art
names(comb) = sub('.txt', '', names(comb))
``` 

## Create a text mining corpus
```{r}
corp_tr=Corpus(VectorSource(comb))
```

## processing and tokenization:
## change alphabet to lower cases
## Removing numbers 
## Removing punctuation
## Removing excess space  
## stop words*  

```{r, echo = FALSE,warning=FALSE}
#processing and tokenization using tm_map function:
corp_tr_cp=corp_tr #copy of the corp_tr file
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(tolower)) #convert to lower case
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(removeNumbers)) #remove numbers
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(removePunctuation)) #remove punctuation
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(stripWhitespace)) #remove excess space
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(removeWords),stopwords("en")) #removing stopwords. Not exploring much on this, to avoid losing out on valuable information.
DTM_train = DocumentTermMatrix(corp_tr_cp)
DTM_train # some basic summary statistics
#Removing sparse items
DTM_tr=removeSparseTerms(DTM_train,.99)
tf_idf_mat = weightTfIdf(DTM_tr)
DTM_trr<-as.matrix(tf_idf_mat) #Matrix
tf_idf_mat #3394 words, 2500 documents
```

## Repeat for the test directories**  

## Read Test files

```{r}
test=Sys.glob('C:/Users/Lai Jiang/Desktop/UT MSBA Summer Courses/Predictive Modeling/STA380-James Scott/STA380-master/data/ReutersC50/C50test/*')
```

```{r}
comb_art1=NULL
labels1=NULL
for (name in test)
{ 
  author1=substring(name,first=50)
  article1=Sys.glob(paste0(name,'/*.txt'))
  comb_art1=append(comb_art1,article1)
  labels1=append(labels1,rep(author1,length(article1)))
}
``` 

## cleaning the file name
```{r}
comb1 = lapply(comb_art1, readerPlain) 
names(comb1) = comb_art1
names(comb1) = sub('.txt', '', names(comb1))
```
#Create a text mining corpus
```{r}
corp_ts=Corpus(VectorSource(comb1))
```

## processing and tokenization*  

```{r, echo = FALSE,warning=FALSE,include=FALSE}
#processing and tokenization using tm_map function:
corp_ts_cp=corp_ts #copy of the corp_tr file
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(tolower)) #convert to lower case
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removeNumbers)) #remove numbers
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removePunctuation)) #remove punctuation
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(stripWhitespace)) #remove excess space
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removeWords),stopwords("en")) #removing stopwords. Not exploring much on this, to avoid losing out on valuable information. 
```


## making sure we have the same test and training sets 

```{r, echo = FALSE,warning=FALSE}
# making sure the same number of variables in both data sets by specifying column names from the train document term matrix
DTM_ts=DocumentTermMatrix(corp_ts_cp,list(dictionary=colnames(DTM_tr)))
tf_idf_mat_ts = weightTfIdf(DTM_ts)
DTM_tss<-as.matrix(tf_idf_mat_ts) #Matrix
tf_idf_mat_ts #3394 words, 2500 documents
```

## Reducing dimentionality 

## Using PCA to extract relevant features from the huge set of variables and eliminate the effect of multicollinearity

## Data Preparation for PCA

## Eliminate NA columns

```{r}
DTM_trr_1<-DTM_trr[,which(colSums(DTM_trr) != 0)] 
DTM_tss_1<-DTM_tss[,which(colSums(DTM_tss) != 0)]
```

## only using intersecting columns
```{r}
#8312500 elements in both. 
DTM_tss_1 = DTM_tss_1[,intersect(colnames(DTM_tss_1),colnames(DTM_trr_1))]
DTM_trr_1 = DTM_trr_1[,intersect(colnames(DTM_tss_1),colnames(DTM_trr_1))]
```


## apply PCA function
```{r}
mod_pca = prcomp(DTM_trr_1,scale=TRUE)
pred_pca=predict(mod_pca,newdata = DTM_tss_1)
```
## choosing # of PC
##75% variance explained at PC724

```{r}
# 724 out of 2500 principal components since 75% varaince explained
plot(mod_pca,type='line') 
var <- apply(mod_pca$x, 2, var)  
prop <- var / sum(var)
#cumsum(prop)
plot(cumsum(mod_pca$sdev^2/sum(mod_pca$sdev^2)))
```

## Preparing data for classification models 

```{r}
tr_class = data.frame(mod_pca$x[,1:724])
tr_class['author']=labels
tr_load = mod_pca$rotation[,1:724]
ts_class_pre <- scale(DTM_tss_1) %*% tr_load
ts_class <- as.data.frame(ts_class_pre)
ts_class['author']=labels1
```

## random forest

```{r, echo = FALSE,warning=FALSE,include=FALSE}
library(randomForest)
set.seed(999)
mod_rand<-randomForest(as.factor(author)~.,data=tr_class, mtry=6,importance=TRUE)
```


## What accuracy does Random Forest give us? 
##1908 documents is correctly classified by its author; accuracy rate of 76.32%*  
## test took about 15 min on my computer.... 

```{r}
pre_rand<-predict(mod_rand,data=ts_class)
tab_rand<-as.data.frame(table(pre_rand,as.factor(ts_class$author)))
predicted<-pre_rand
actual<-as.factor(ts_class$author)
temp<-as.data.frame(cbind(actual,predicted))
temp$flag<-ifelse(temp$actual==temp$predicted,1,0)
sum(temp$flag)
sum(temp$flag)*100/nrow(temp)
```
## Naive Bayes  

## Fit the training set and test on testing set*  

```{r}
library('e1071')
mod_naive=naiveBayes(as.factor(author)~.,data=tr_class)
pred_naive=predict(mod_naive,ts_class)
``` 

## Classification accuracy obtained:
## 810 documents are correctly classified by its authors, accuracyrate of 32.4%*  
## took 1min to run this test

```{r}
library(caret)
predicted_nb=pred_naive
actual_nb=as.factor(ts_class$author)
temp_nb<-as.data.frame(cbind(actual_nb,predicted_nb))
temp_nb$flag<-ifelse(temp_nb$actual_nb==temp_nb$predicted_nb,1,0)
sum(temp_nb$flag)
sum(temp_nb$flag)*100/nrow(temp_nb)
```

## Comparing train and test accuracy

```{r, echo = FALSE,warning=FALSE,include=FALSE}
pred_naive_tr=predict(mod_naive,tr_class)
tr_err_naive_pre<-pred_naive
```


## Random Forest provides the best accurary at 74%, while Naive Bayes have accurary around 32%

############################################################################################################################################################################################################################################################################################

## Association Rule Mining 

```{r}
library(tidyverse)
library(arules)
library(arulesViz)
```

```{r}
groceries_raw = scan("https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt", what = "", sep = "\n")
head(groceries_raw)
```

```{r}
str(groceries_raw)
summary(groceries_raw)
```

##there are 9835 transactions

```{r echo=FALSE, include=FALSE}
## Cast this variable as a special arules "transactions" class.
groceries = strsplit(groceries_raw, ",")
groctrans = as(groceries, "transactions")
summary(groctrans)
```

##most frequent items are milk, veggie, buns, soda and yogurt in decreasing order



```{r}
# Now run the 'apriori' algorithm
# Look at rules with support > .05 & confidence >.1 & length min is 2
groctranscrules = apriori(groctrans, 
	parameter=list(support=.05, confidence=.1, minlen=2))
```
```{r}
arules::inspect(groctranscrules)
plot(groctranscrules, method='graph')
```

## We visualize 6 rules. We see most linkage involing veggie, wholemilk, rolls and yogurts. This makes sense since they are most frequently bought.

## decrease support to 0.03 and increase confidence to 0.2, min len still 2 

```{r}
groctranscrules1 = apriori(groctrans, 
                     parameter=list(support=0.03, confidence=.2, minlen=2))
arules::inspect(groctranscrules1)
```



```{r}
plot(head(groctranscrules1,15,by='lift'), method='graph')
```
## This graph contain 15 rules, and include a little bit more items, however we can see milk still at the center with most assocation, there is a micro center around veggies


```{r}
groctranscrules2 = apriori(groctrans, 
                     parameter=list(support=0.01, confidence=.3, minlen=2))
arules::inspect(groctranscrules2)
```


```{r}
plot(head(groctranscrules2,15,by='lift'), method='graph')
```



## Conclusions:
##People likely to purchase sausages if they purchase rolls and buns
##root veggies and other veggies are associated together
##milk is the most common item bought by customers
##when you purchase citrus fruits and tropical fruits, you will buy veggies







#########################################TheEnd############################################################################



Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
